[["index.html", "Data science for the liberal arts ", " Data science for the liberal arts Kevin Lanning 2025-01-04 "],["preface.html", "preface the role of the liberal arts in data science some features of the text the book is for you", " preface This work-in-progress will ultimately serve as a textbook for introductory undergraduate courses in data sciences, and is particularly aimed at students in the liberal arts. No prior knowledge of computer programming is presumed, though, ideally, students will have had college algebra (or its equivalent) and an introductory course in statistics, methods, or data analysis. the role of the liberal arts in data science Data science is still a relatively new field of study, and there are multiple approaches to teaching it and to its place in the college curriculum. This book is intended to serve courses such as Introduction to Data Science at the Wilkes Honors College of Florida Atlantic University which, in turn, was initially based on data science classes at the universities of North Carolina, British Columbia, Duke, Maryland, Wisconsin, Stanford, BYU, Harvard, and UC Berkeley. At each of these schools, the Introduction to Data Science is, to my eyes at least, closer to Statistics than to Computer Science. Statistics is itself a broad field, and our approach is aligned with its most applied and pragmatic form. From this perspective, the choice of statistical methods should follow from the data and problem at hand - in other words, statistics should serve the needs of the user rather than dictate them (Loevinger 1957). Pragmatism, in turn, can serve various goals, ranging from maximizing the revenues generated by an online ad to minimizing the carbon footprint of a travel itinerary. Data science for the liberal arts may be seen as a fusion of the pragmatism of data science with social and humanistic concerns; we stand beside programs in Computational Social Science as it has been taught at schools including Chicago, Georgia Tech, UC Santa Barbara, Princeton, UC Berkeley, at Berlin’s Hertie School of Governance, and in Columbia’s School of Journalism. Data science for the liberal arts begins with the person and society rather than with the algorithm and network. In its concern with the liberal arts, it is intended to provide a modest counterbalance to the inherently centripetal, or inequality-accelerating, force of modern information technology.1 some features of the text There are a number of different approaches to teaching data science for the liberal arts. The present text includes several distinguishing features. R In a recent informal survey of introductory data science courses, I saw a pretty even split between those which begin with Python and those which begin with the statistical programming language R. This difference corresponds, very loosely, to the split noted above: Computer science based approaches to data science are frequently grounded in Python, while statistics-based approaches are generally grounded in R. Our course, like those for most of the syllabi and courses linked above, will be based in R. Reproducible science The course will provide an introduction to some of the methods and tools of reproducible science. We will consider the replication crisis in the natural and social sciences, and then consider three distinct approaches which serve as partial solutions to the crisis. The first of these is training in a notebook-based approach to writing analyses, reports and projects (using R markdown documents). The second is using public repositories (such as the Open Science Framework and, to a limited extent, GitHub) to provide snapshots of projects over time. Finally, the third is to consider the place of significance testing in the age of Big Data, and to provide training in the use of descriptive, exploratory techniques of data analysis. Good visualizations As I note in the first chapter, communication is a distinguishing concern of data science for the liberal arts. “Communication” includes not just writing up results, but also designing data displays that incisively convey the key ideas or features in a flood of data. We’ll examine and develop data visualizations such as plots, networks and text clouds. More advanced topics may include maps, interactive displays, and animations. A little data There are plenty of data sources for us to examine, and we’ll consider existing datasets from disciplines ranging from literature to economics to public health, with sizes ranging from a few dozen to millions of data points. We will also clean and create new datasets. A few tools One feature of Data Science is that it is changing rapidly. The tools, methods, data sources, and ethical concerns that face us in 2025 are different from those which shaped the field just one or two years ago. In fields that are undergoing rapid change, there is some trade-off between building expertise with existing (older) tools and trying the newer approaches. Partly because I want to equip you with skills which will not be obsolete, partly because some of these new approaches promise more accessibility, elegance, and/or power, and partly because of my own interest in staying current, we’ll be using some of the latest packages and programs. In the last few years, I’ve shifted the class from the standard R dialect (as I learned it from the Johns Hopkins-Coursera Data Science Specialization) to the Tidyverse, a dialect of R that I find to be relatively clear and concise. A few years ago, I shifted our primary platform from individual laptops to a cloud-based R platform; while this approach has its advantages, I found that these did not outweigh the costs of the approach, so we will go back to the standalone method. We’ll explore different approaches to learning R syntax, including the learnr package, Swirl, and DataCamp. In the past, I’ve recommended using dedicated markdown editors such as Typora and Obsidian. While I still think that these are worth considering for some text-editing and note-taking applications, we’ll do our work instead with the editor in the latest variant of RStudio on our laptops, as this allows WYSIWIG (what you see is what you get) formatting of documents - such as this one - that are intended as “publication-ready” texts. We’ll use, and explore the advantages and disadvantages, of spreadsheets such as Excel or Google Sheets as well. the book is for you It’s my intention that this text should serve every college student, regardless of concentration or college major. The skills and insights that you will gain in this course will help you in graduate and professional schools, will help you in your careers, and will help you in your goal of making a better world. And it will help you train the next generation of data scientists as well. References Loevinger, Jane. 1957. “Objective Tests as Instruments of Psychological Theory.” Psychological Reports 3 (3): 635–94. https://doi.org/b27jpk. Watts, Duncan J. 2004. “The \"New\" Science of Networks.” Annual Review of Sociology 3: 243–70. https://www.jstor.org/stable/29737693. I hope to return to this in a later chapter, but in the meantime consider the discussion of the “Matthew Effect” in sociology and network science (Watts 2004).↩︎ "],["what-is-data-science-for-the-liberal-arts.html", "1 what is “data science for the liberal arts?” 1.1 the incompleteness of the data science Venn diagram 1.2 Google and the liberal arts 1.3 TMI 1.4 discussion: what are your objectives in data science?", " 1 what is “data science for the liberal arts?” Hochster, in (Hicks and Irizarry 2018), describes two broad types of data scientists: Type A (Analysis) data scientists, whose skills are like those of an applied statistician, and Type B (Building) data scientists, whose skills lie in problem solving or coding, using the skills of the computer scientist. This view arguably omits a critical component of the field, as data science is driven not just by statistics and computer science, but also by “domain expertise:” Fig 1.1 - The iconic data science Venn diagram The iconic Venn diagram model of data science, as shown above, suggests that there are not two but three focal areas in the field, one of which begins not with math or computer science, but with “domain expertise.” Data science for the liberal arts is a ‘Type C’ approach, where ‘C’ refers to a concentration of concern in the arts, humanities, social and/or natural sciences. For the Type C data scientist, coding is in the service of applied problems and concerns. Type C data science does not merely integrate ‘domain expertise’ with statistics and computing, it places content squarely at the center. We can appreciate the compelling logic and power of statistics as well as the elegance of well-written code, but for our purposes these are means to an end. Programming and statistics are tools in the service of social and scientific problems and cultural concerns. Type C data science aims for work which is not merely cool, efficient, or elegant but responsible and meaningful. At the risk of oversimplifying: Type A data scientists focus on Analysis and questions about ‘how?’ Type B data scientists focus on Building and questions of ‘what?’ Type C data scientists focus on Consideration and questions of ‘why?’, ‘who?’, ‘what for?’, and ‘at what (social) cost?’ 1.1 the incompleteness of the data science Venn diagram Data visualizations are starting points which can provide insights, typically highlighting big truths or effects by obscuring other, presumably smaller ones. The Venn diagram model of data science is no exception: As with other graphs, figures, and maps, it allows us to see by showing only part of the picture. What does it omit? That is, beyond mathematics, computing, and domain expertise, what other skills contribute to the success of the data scientist? 1.1.1 additional domains For the liberal arts data scientist, we can note at least three additional important domains, that is, communication, collaboration, and citizenship. Communication, including writing and the design and display of quantitative data, is central to data science because results are inconsequential unless they are recognized, understood, and built upon. Facets of communication include oral presentations, written texts and good data visualizations. Collaboration is important because problems in data science are sufficiently complex so that any single individual will typically have expertise in some, but not all, facets of the area. Collaboration, even more than statistical and technical sophistication, is arguably the most distinctive feature of contemporary scholarship in the natural and social sciences as well as in the private sector (Isaacson 2014). Citizenship is important because we are humans living in a social world; it includes serving the public good, overcoming the digital divide, furthering social justice, increasing public health, diminishing human suffering, and making the world a more beautiful place. The Type C data scientist is aware of the fact that the world and workforce are undergoing massive change: This puts the classic liberal arts focus of “learning how to learn” (as opposed to memorization) at center stage. Finally, the Type C data scientist is sensitive to the creepiness of living increasingly in a measured, observed world. These real-world goals should be informed by ethical concerns including a respect for the privacy and autonomy of our fellow humans. Reproducibility is related to communication, collaboration, and citizenship. There has been something of a crisis in recent years in the social and natural sciences as many results initially characterized as “statistically significant” have been found not to replicate. The reasons for this are multiple and presently contentious, but one path towards better science includes the public sharing of methods and data, ideally before experiments are undertaken. With the large datasets that are often available in contemporary data science, statistical significance is rarely a concern and other indices, including measures of effect size, are typically of primary interest. 1.1.2 an additional dimension Cutting across these various facets (statistics, computing, domain expertise, collaboration, communication, and citizenship), a second dimension can be articulated. No one of us can excel in all of these domains, rather, we might aim towards a hierarchy of goals ranging from literacy (can understand) through proficiency (can get by) to fluency (can practice) to leadership (can create new solutions or methods). That is, we can think of a continuum of knowledge, skills, interests, and goals, ranging from that which characterizes the data consumer to the data citizen to the data science contributor. A Type C data science includes this dimension of ‘depth’ as well. 1.2 Google and the liberal arts Data science is at its core empirical, and all of this rhetoric would be meaningless if not grounded in real world findings. Although it was reported in late 2017 that soft skills rather than STEM training were the most important predictors of success among Google employees, it’s difficult to know whether these results would generalize to a less select group. Nonetheless, there is a clear need for individuals with well-rounded training in the liberal arts in data science positions and, conversely, learning data science is arguably a key part of a contemporary liberal arts education. 1.3 TMI A primary difference between traditional statistics and data science is that the former is typically concerned with making inferences from datasets that are too small, while the latter is concerned with making sense of data that is or are too big (D. Donoho 2017). Data science is fundamentally concerned with the problem of “too much information,” or TMI. In one sense, the idea of TMI is not new. In the 19th Century, Wilhelm Wundt argued that attention was the distinguishing act of the human mind (Blumenthal 1975). In attending to (or focusing on) something, we must overlook everything else, consequently, selection is the essence of human perception (Erdelyi 1974). Selection is important not just in psychology, but in the arts as well, for editing, or choosing what not to write or show, is at the core of the creation of works including novels and film (Ondaatje and Murch 2002). So the problem of TMI is not new. Yet today, this problem exists at a much greater scale, for there is simply more information around us. Indeed, over the last 20 years, the amount of digital information in the world has increased roughly 200-fold.2 Like perception in psychology and editing in the arts, data science is concerned with extracting meaning from information. Because the amount of information around us has mushroomed and its nature has become more important, our need to extract meaning has become more ubiquitous and more urgent. For these reasons, data science is a foundational discipline in 21st century inquiry. 1.4 discussion: what are your objectives in data science? References Blumenthal, Arthur L. 1975. “A Reappraisal of Wilhelm Wundt.” American Psychologist 30 (11): 1081. Donoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734. Erdelyi, Matthew H. 1974. “A New Look at the New Look: Perceptual Defense and Vigilance.” Psychological Review 81 (1): 1–25. https://doi.org/cs5c5q. Hicks, Stephanie C., and Rafael A. Irizarry. 2018. “A Guide to Teaching Data Science.” The American Statistician 72 (4): 382–91. https://doi.org/gfr5tf. Isaacson, Walter. 2014. The Innovators: How a Group of Hackers, Geniuses, and Geeks Created the Digital Revolution. Simon and Schuster. Ondaatje, Michael, and Walter Murch. 2002. The Conversations: Walter Murch and the Art of Editing Film. A&amp;C Black. Reinsel, David, John Gantz, and John Rydning. 2025. “The Digitization of the World.” According to Wikipedia, the Zettabyte (ZB) Era began in 2012, when the amount of digital information in the world first exceeded 1 ZB (or 1021 bytes). In 2025, it is estimated that the world will house 175 ZBs of digital data (Reinsel, Gantz, and Rydning 2025), hence a 175X increase in in 13 years. My estimate of a 200X increase in 20 years is a conservative extrapolation from these numbers. Incidentally, one ZB = 1,000,000,000,000,000,000,000 bytes, which could be stored on roughly 250 billion DVDs, or 500 million 2 TB hard drives.↩︎ "],["getting-started.html", "2 getting started 2.1 are you already a programmer and statistician? 2.2 spreadsheets - some best practices 2.3 setting up your machine: some basic tools 2.4 a (modified) 15-minute rule 2.5 installing R and RStudio desktop", " 2 getting started We begin with a brief self-assessment, asking you to reflect on your own knowledge of data science, including the necessary-but-not-sufficient areas of computer programming and statistics. We then move to a description of some rudimentary tools that we will be using, then install the R programming environment on our laptops. 2.1 are you already a programmer and statistician? Regarding programming, you may know more than you think you do. Here’s a simple program - a set of instructions - for producing a cup of coffee: add water to the kettle and turn it on if it’s morning, put regular coffee in the French press, otherwise use decaf if the water has boiled, add it to the French press, else keep waiting if the coffee has steeped for four minutes, depress (smash) piston/plunger, else keep waiting pour coffee into cup enjoy As a post-millennial student from a WEIRD culture, or Western, Educated, Industrialized, Rich Democracy (Henrich, Heine, and Norenzayan 2010), you’ve ‘programmed’ computers, too, if only to enter a password, open an app, and upload a photo on your cell phone. Statistics is of fundamental importance, not just for understanding abstract trends, but for making decisions about everyday life. Consider the case of a senior undergraduate who wants to go to med school. How many schools should she apply to? Exercise 2_1 Susie is applying to two med schools. At School A, 25% of students are accepted, and at School B, 25% are accepted as well. You are Susie. Are you going to get in to at least one of these programs? What is the probability? Does your estimate depend upon any assumptions? Questions such as these are important for us. If the combined probability is low, it will likely (another probability concept) make sense for Susie to spend the time, money, energy, and ego-involvement to apply to additional programs. If the probability is higher, it may not. But problems like this are hard - our estimates of probability are frequently poorly calibrated, and combining probability estimates is challenging (see, e.g., (Tversky and Kahneman 1974), and consider taking a course in Behavioral Economics or Thinking and Decision Making to learn more). 2.2 spreadsheets - some best practices Spreadsheets are handy tools, particularly for smaller datasets. You may have worked with data in spreadsheets such as Microsoft Excel or Google Sheets. If you haven’t here’s a start: Exercise 2_2 Open the Google Sheet at http://bit.ly/dslaX2_1. Save a copy and edit it, entering the following in cell B7: =SUM (B2:B6) What is the result? Now copy cell B7 to C7 What happens? Is this the result you expected? Spreadsheets are great tools - the first one, VisiCalc, was the first “killer app” to usher in the personal computer revolution. But spreadsheets have limitations as well. Best practices have been proposed for using spreadsheets in data science such as, for example, including only data (and not calculations) in spreadsheets, using what we will recognize as a ‘tidy’ format in which data are in a simple rectangle (that is, avoid combining cells and using multi-line headers), and saving spreadsheets as simple text files, typically in comma-delimited or CSV format (Broman and Woo 2018). Typically, this means that each worksheet will be in a separate file, rather than in workbooks with multiple tabs. There are good reasons for these recommendations: When we manipulate data in spreadsheets, we typically will not have a record of what was (and wasn’t) changed, and this compromises the reproducibility of our work. Similarly, when we sort data in spreadsheets, we risk chaos, for example, if we sort only certain columns, the integrity of spreadsheet-rows will be lost. In general, spreadsheets should generally be used to store data rather than to analyze it. But don’t be a slave to this - if you are making some quick calculations that are inconsequential and/or to be used on just one occasion, working in Excel or Google Sheets is often the way to go. 2.3 setting up your machine: some basic tools Documents produced in word processors typically include invisible characters used for formatting, security, etc. These characters, when fed into a computer program, lead to unpredictable results. Markdown (MD) is a simple syntax for producing documents that are easily read by both humans and machines. Most of the documents you will produce and work with this term, including the present chapter, are and will be written in some version of Markdown. Three markdown editors are worth considering. The newest is Obsidian, which is free and includes an extended version of Markdown that allows the drawing of simple network graphs. Typora has an attractive minimalist interface, but is no longer free ($15). Finally, RStudio - the environment within which we will be using R - has a built in visual markdown editor as well. Collaboration and communication are integral to data science. In the world beyond universities, the most important messaging and collaboration platform is Slack. Slack is a commercial app, but it has a free tier. Slack is handy for group work of all forms, but most universities use collaborative tools embedded in Learning Management Systems such as Canvas instead. While Microsoft Word has the advantages of familiarity, ease-of-use offline, and extensive formatting capabilities, Google Docs has several advantages over Word. Google Docs is free, it is convenient for collaborative work (as it allows simultaneous editing), and it provides a solid framework for version control, a critical skill in information management. Version control can help you avoid the chaos and confusion of having a computer (or several computers) full of files that look like Cham’s (2012) comic: Fig 2.1: Never call anything ‘final.doc’. Version control is an important concept in data science. Collaboratively built programs and platforms, including most of the add-ons (libraries, packages) which make R so powerful, are open-source projects built by many individuals over time. For projects such as these, both the current build and its associated history are typically maintained on GitHub, a website for hosting code. When we contribute to these projects, we will first mirror the web-based GitHub site using a program on our own Macs or Windows PCs called Git, then upload our proposed changes. Keeping remote and local branches of files in sync can be challenging, however, and you will not be expected to use this technology in this class. But if you are curious, or want to learn more, an introduction to using Git and R together may be found here. 2.4 a (modified) 15-minute rule While AI tools for coding, including Microsoft Copilot, are helpful for addressing the idiosyncacies of coding syntax, at some point, you will run into problems - if you don’t you aren’t learning enough. An important determinant of your success will be the balance you maintain between persistence and help-seeking. The 15-minute rule is one guideline for this balance: It has been cleverly summarized as “You must try, and then you must ask.” That is, if you get stuck, keep trying for 15 minutes, then reach out to others. I think that this rule is basically sound, particularly if it is applied with cognitive flexibility, social sensitivity, and reciprocity. So when you get stuck, make a note of the problem, then move to another part of your project (that’s the cognitive flexibility part): This allows your problem to percolate and still make progress. When you ask others for help, ask in a way that shows an awareness of the demands on their time (social sensitivity): Part of this means that you should explain your problem in as detailed a fashion as possible - in technical terms, a “reprex” or reproducible example. Finally, you should be willing to provide as well as give help (reciprocity). 2.5 installing R and RStudio desktop Finally, if you have not already done so, install the latest version of R on your own Windows or Mac laptop. Then install RStudio Desktop. (RStudio is our interface, the environment we will use to write, test, and run R code). References Broman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989. Henrich, Joseph, Steven J. Heine, and Ara Norenzayan. 2010. “The Weirdest People in the World?” Behavioral and Brain Sciences 33 (2-3): 61–83. https://doi.org/c9j35b. Tversky, Amos, and Daniel Kahneman. 1974. “Judgment Under Uncertainty: Heuristics and Biases.” Science 185 (4157): 1124–31. https://doi.org/gwh. "],["what-r-stands-for.html", "3 what R stands for … 3.1 some key characteristics of R 3.2 cha-cha-cha-changes 3.3 some technical characteristics 3.4 finding help 3.5 Wickham and R for Data Science 2e", " 3 what R stands for … R was initially developed by Ross Ihaka and Robert Gentleman as a tool to help teach university-level statistics at the University of Auckland. At one level, the name ‘R’ simply stands for the first initial of these two founders (Hornik and Team 2022). But, just as we noted that the ‘C’ in Type C Data Analysis stands for concepts such as concentration, communication, collaboration, the ‘R’ in our programming language means much more: R is a system for reproducible analysis, and reproducibility is essential. When we write R code, we’ll use R markdown documents. An R markdown document can include text (comments or explanations), ‘chunks’ of code, and output including graphs and tables. Having explanations, code, and results in a single document facilitates reproducible work. (Jupyter notebooks in the Python world are similar in this respect). R is for research. Research is not just an end-product, not just a published paper or book: … these documents are not the research [rather] these documents are the “advertising”. The research is the “full software environment, code, and data that produced the results” (Buckheit and Donoho 1995; D. L. Donoho 2010, 385). Published works (including theses as well as books, scholarly papers, and business reports) are summaries; R markdown documents are the raw materials from which these are derived. When we consider only summaries (or the ‘advertising’), we make it difficult for others to verify, or build upon, the findings by reproducing them (Gandrud 2013). R is a system for representing data in cool, insight-facilitating ways, a tool for creating (reproducible) data visualizations which can provide insights and communicate results. The power of R to make clear, honest, and reproducible data visualizations is widely seen as a major strength of the language. R is really popular, and this matters, because learning R will make you a more attractive candidate for many graduate programs as well as jobs in the private sector. Because R is popular, there are many resources, including, for example - Online resources include the simple (and less simple) lessons of SwirlR, which offers the possibility of “learning R in R,” as well as DataCamp, the Data Science Certificate Program at Johns Hopkins, and other MOOCs. Books include (Peng 2014) - which includes not only videos of his lectures in the program at Hopkins, but also a brief list of still more resources - and (Wickham, Çetinkaya-Rundel, and Grolemund 2023) You’ll also learn (more directly) from people, including your classmates, as well as the broader community of people around the world. There are hundreds if not thousands of people, young and old, who are on the road with you. I am as well, just a step or two (hopefully) ahead. R might stand for relatively high level. Programming languages can be described along a continuum from high to low level, the former (like R) are more accessible to humans, the latter (like assembly language) more accessible to machines. Python, Java, and C++ are all more towards the middle of this continuum. R does not stand for ‘arggh,’ although you may proclaim this in frustration (‘arggh, why can’t I get this to work?) or, perhaps, in satisfaction (’arggh, matey, that be a clever way of doing this’).3 But R does stand for rewarding. A language is a way of thinking about the world, and this is true for computer languages as well. You’ll be challenged by its complexity, its idiosyncracy, its alien logic. But you will succeed, and you will find that you can do things that you did not believe possible. 3.1 some key characteristics of R 3.1.1 base R and packages R is a programming language. It can be seen as including two parts, a simple core (Base R) and a large number of additional packages. These packages (libraries) are customized add-ons which simplify certain tasks, such as text analysis. There are, at this writing, 21,861 available packages on the CRAN package repository (as well as additional useful packages that, for one reason or another, do not appear on CRAN. Packages on CRAN are partially indexed by “task view pages.” The task view page for natural language processing or text analysis includes, at this writing, over 60 separate packages. So how do you choose, and where do you begin? For our purposes, we will start with the curated list of packages which jointly comprise the tidyverse (Wickham et al. 2019), which is effectively a dialect of R. To download the tidyverse package from the ‘net, open RStudio, find the ’console’ window on the left side of your screen, and enter the command followed by &lt;enter&gt; or &lt;return&gt; install.packages(“tidyverse”) 3.2 cha-cha-cha-changes R is constantly changing, not just in the proliferation of packages, but also in the organization of the R community. While R is free and open source, RStudio is a commercial product. The company (and website) that develops the RStudio IDE is undergoing a name change (from RStudio to Posit). This is motivated, in part, by the need to make the RStudio platform more welcoming for other languages including Python. Similarly, the R markdown programming language is slowly being replaced with newer, and ultimately more capable, software called Quarto. Quarto is back-compatible with R markdown, but can be used with other languages including Python as well. A description of the differences between R markdown and Quarto may be found here. For our purposes, you can treat Quarto files (.qmd suffix) as R markdown files (.rmd), and vice-versa. One more change: Posit (the company) is developing a new IDE called ‘Positron.’ Positron may ultimately be a more useful environment for data science than the RStudio IDE, but it is in the beta testing stage at this writing. The RStudio environment, and the Rmarkdown documents that are produced within it, will continue to be available, and widely used, for the foreseeable future. 3.3 some technical characteristics R is an object-oriented language - one conceptually organized around objects and data rather than actions and logic. In R, at the most basic or atomic level, “objects” include characters, real numbers, integers, complex numbers, and logicals. These atomic objects may be combined into vectors, which generally include objects of the same type [one kind of object, ‘lists,’ is an exception to this; Peng (2014)]. Vectors can be further combined into data frames, which are two-dimensional tables or arrays. A tibble is a particular type of data frame which is used in the tidyverse. Tibbles are, in some ways, handier to work with than other data frames. We’ll be working extensively with data frames in general, and tibbles in particular, as we move forward. Objects have attributes. Attributes of R include such things as name, dimensions (for vectors and arrays), class (that’s the type of object described in the previous paragraph), length, etc. Real world data sets are messy, and frequently have missing values. In R, missing values may be represented by NA (not available) or NaN (not a number, implying an undefined or impossible value). 3.4 finding help One does not simply ‘learn R.’ Unlike, say, learning to ride a bicycle, fry an egg, or drive a car with a manual transmission, learning R is not a discrete accomplishment that one can be said to have mastered and from which one then moves on. Rather, R is an evolving, open system of applications and tools which is so vast that there is always more that one can achieve, new lessons that one can learn. And, the complexity of R syntax is such that, for almost all of us, we will need help for coding on any non-trivial task. For us, the key ideas in “looking for help” will include not just the tools on the RStudio IDE, but also (a) using Google searches wisely, (b) judicious use of AI assistance, and (c) reaching out to your classmates and instructor. Here, as in the real world, there is an etiquette for help-seeking which is based on consideration. Your search for help should begin by making sure that others will encounter the same result, then by stripping the problem down to its essence. Once you have reduced the problem to this minimal, reproducible essence, you will often be able to spot the problem yourself - and, if not, you will make it easier for others to help you. There is an R package (reprex) which will likely facilitate this, but I haven’t tried it yet. Here is a good introduction. Again, remember the 15 minute rule - we all need somebody to lean on. Finally, to get a sense of some of the ways you can get help in RStudio (and to see how a master uses the R Studio interface), consider this video: video 3.1: Garrett Grolemund of RStudio 3.5 Wickham and R for Data Science 2e The introduction to the Wickham text (Wickham, Çetinkaya-Rundel, and Grolemund 2023) provides both a framework for his approach and a brief introduction to the tidyverse. Please read it now. References Buckheit, Jonathan B., and David L. Donoho. 1995. “Wavelab and Reproducible Research.” In Wavelets and Statistics, 55–81. Springer. Donoho, David L. 2010. “An Invitation to Reproducible Computational Research.” Biostatistics 11 (3): 385–88. https://doi.org/bxwkns. Gandrud, Christopher. 2013. Reproducible Research with R and R Studio. CRC Press. Hornik, Kurt, and The R Core Team. 2022. “R FAQ: The Comprehensive R Archive Network.” Peng, Roger D. 2014. R Programming for Data Science. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". Actually, pirates have little use for R, as pirates love the C (programming language).↩︎ "],["exploring-r-world.html", "4 exploring R world 4.1 go to the movies 4.2 go into the clouds 4.3 open the box 4.4 go to (data)camp 4.5 learn to knit 4.6 older approaches", " 4 exploring R world There are many sources for learning the basics of R. A few of these follow. Please spend at least 180 mins exploring at least two of the following. Be prepared to discuss your progress next class: You will be asked which source(s) you used, what you struggled with, what questions you have, and what you would recommend to your classmates. Hint: If you find the material too challenging, remember the 15 minute rule, take a break away from your machine and other screens, clear your head, then try a different approach. 4.1 go to the movies About seven years ago, Iain Carmichael used data from the Internet Movies Database (IMDB) to introduce R. You can see his introduction here. You can consider his report from the standpoint of style (formatting, organization), coding (how he did this), data (the part of the IMDB data he is looking at), and his results (plots of distributions and relationships). Do you have any questions about the movie data? How might you ask these? A minimal amount of sleuthing - a click on the STOR 390 link at the top of the page, then a quick scroll - reveals that “all of [Carmichael’s] course material is on the github repo” - or repository. Can you find the Rmd document that generated his work? Can you download it on to your machine? If you try to run it, what happens? If you were working on your thesis and came across a problem like this, what would you do next? 4.2 go into the clouds In addition to the desktop version of R (and Rstudio) we will be using in this class, there is a cloud-based environment as well. Go to the website posit.cloud, click on “learn more” in the “Free” column, then sign up. When you open posit.cloud, you should see a column on the left of the screen that includes four sections - spaces, learn, help, and info. (If you don’t see these, click on the hamburger menu in the upper left corner and it will appear). Browse through the recipes tab, particularly the ones in the left most column, to start to get a sense of how you might solve some common challenges in R. 4.3 open the box Go to datasciencebox/content. Click on the “Hello world” link; this will take you to the beginning of Mine Çetinkaya-Rundel’s introductory lessons on R. These include slides, the source code for the slides (these are written in R markdown), and videos of her lectures, including the one we watched on the first day of class. Midway down this page, you’ll find a link to “the RStudio Cloud workspace for Data Science Course in a Box project.” Open it up and begin to explore the code and data behind her presentation. 4.4 go to (data)camp Many folks swear by (and others, I presume, at) DataCamp, which kind of gamifies learning software. As a student in this class, you have access to all of their stuff… free. You can even do lessons on your phone. Use the link given to you in class to enroll, then explore the introductory-level classes in R at https://www.datacamp.com/category/r 4.5 learn to knit In literate programming, comments, code, and results are integrated in a clear and reproducible way - they document our work. ‘Markdown’ is a simple language for adding formatting to text. ‘R’ is a statistical language. ‘R Markdown’ is a variant of R that you can use to produce or publish complex documents like this one, as well as the Carmichael page described above. To create an R markdown (Rmd) document, open up Rstudio, click on (File -&gt; New File -&gt; R Markdown). A window will open up with a file that begins with a block of YAML (Yet Another Markdown Language). You can edit this as needed: --- title: &quot;Here&#39;s an R Markdown Document&quot; author: &quot;Frankie McFrank Frank&quot; date: &quot;1/12/2025&quot; output: html_document --- Go ahead and click on the clever “knit” icon in the bar just above the source window to create a sample document. You’ll need to save the file with a new name, then R will finally create an HTML (Hyper Text Markdown Language ) page. Compare the R Markdown document (your code) with the result (the HTML). The second chapter of Healy’s online book about Data visualizations provides a more thorough explanation of R Markdown as well as an introduction to R and R studio which largely parallels the discussions here and in the Wickham and Grolemund text (Healy 2017; Wickham, Çetinkaya-Rundel, and Grolemund 2023). 4.6 older approaches I, like thousands of others, learned R in the process of completing the Johns Hopkins Data Science Specialization offered through Coursera. The sequence can be challenging, but their introduction to R used an accessible, interactive R package called Swirl. You can read about swirl (“learn R in R”) at https://swirlstats.com/. 4.6.1 using Swirl After loading R (and opening R studio), you will get to the Swirl lessons with the following steps: Install the Swirl package on your computer (you only need to do this once). Type the following into your console window in R studio (typically left hand side of your screen or lower left) install.packages(“swirl”) Then load the package into your workspace (you’ll need to do this at the beginning of every session you use Swirl) library (swirl) Then run it! swirl () Swirl will ask a few questions then give you the option of choosing one of several courses. You’ll choose the R Programming option, which leads to 15 separate lessons. At the end of each lesson, you’ll be asked Would you like to receive credit for completing this course on Coursera.org? Answer no… then do another lesson. 4.6.2 reading/watching Roger Peng’s text and/or videos Finally, you might consider the text and videos from the Coursera R class. Most of the material from that class can be found in Roger Peng’s (2014) text, a slightly updated version of which can be found here. The videos in the series may be found in this playlist. Here’s an introduction: . Video 4.1: Roger Peng introducing R References Healy, Kieran. 2017. “Data Visualization for Social Science: A Practical Introduction with r and Ggplot2.” Peng, Roger D. 2014. R Programming for Data Science. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". "],["references.html", "References", " References Blumenthal, Arthur L. 1975. “A Reappraisal of Wilhelm Wundt.” American Psychologist 30 (11): 1081. Broman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989. Buckheit, Jonathan B., and David L. Donoho. 1995. “Wavelab and Reproducible Research.” In Wavelets and Statistics, 55–81. Springer. Donoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734. Donoho, David L. 2010. “An Invitation to Reproducible Computational Research.” Biostatistics 11 (3): 385–88. https://doi.org/bxwkns. Erdelyi, Matthew H. 1974. “A New Look at the New Look: Perceptual Defense and Vigilance.” Psychological Review 81 (1): 1–25. https://doi.org/cs5c5q. Gandrud, Christopher. 2013. Reproducible Research with R and R Studio. CRC Press. Healy, Kieran. 2017. “Data Visualization for Social Science: A Practical Introduction with r and Ggplot2.” Henrich, Joseph, Steven J. Heine, and Ara Norenzayan. 2010. “The Weirdest People in the World?” Behavioral and Brain Sciences 33 (2-3): 61–83. https://doi.org/c9j35b. Hicks, Stephanie C., and Rafael A. Irizarry. 2018. “A Guide to Teaching Data Science.” The American Statistician 72 (4): 382–91. https://doi.org/gfr5tf. Hornik, Kurt, and The R Core Team. 2022. “R FAQ: The Comprehensive R Archive Network.” Isaacson, Walter. 2014. The Innovators: How a Group of Hackers, Geniuses, and Geeks Created the Digital Revolution. Simon and Schuster. Loevinger, Jane. 1957. “Objective Tests as Instruments of Psychological Theory.” Psychological Reports 3 (3): 635–94. https://doi.org/b27jpk. Ondaatje, Michael, and Walter Murch. 2002. The Conversations: Walter Murch and the Art of Editing Film. A&amp;C Black. Peng, Roger D. 2014. R Programming for Data Science. Reinsel, David, John Gantz, and John Rydning. 2025. “The Digitization of the World.” Tversky, Amos, and Daniel Kahneman. 1974. “Judgment Under Uncertainty: Heuristics and Biases.” Science 185 (4157): 1124–31. https://doi.org/gwh. Watts, Duncan J. 2004. “The \"New\" Science of Networks.” Annual Review of Sociology 3: 243–70. https://www.jstor.org/stable/29737693. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science 2e. \" O’Reilly Media, Inc.\". "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
